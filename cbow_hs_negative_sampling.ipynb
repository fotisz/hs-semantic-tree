{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from gensim.models import KeyedVectors\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from gmm_tree import gmm_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Nov. 2013 dump of Wikipedia, only articles with at least 20 pageviews, leaving 460k documents. \n",
    "train_file = '/home/matthias/data/wikipedia.txt'\n",
    "# Checkout and build https://github.com/mleimeister/fastText/tree/hs_precomputed_tree\n",
    "fasttext = '/home/matthias/fastText/fasttext'\n",
    "# Directory to store the trained vectors\n",
    "save_path = '/home/matthias/tmp/'\n",
    "# Questions file for the word analogy task\n",
    "questions_file = '/home/matthias/data/questions-words.txt'\n",
    "# File for word similarity task\n",
    "dataPath = '/home/matthias/data/rw.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set the dimensions of the embedding vectors, for each there will be a separate training and evaluation run\n",
    "dims = [50, 100, 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_formated_time(elapsed, outfile=None):\n",
    "    hours, rem = divmod(elapsed, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    s = 'Elapsed time: {:0>2}:{:0>2}:{:05.2f}'.format(int(hours),int(minutes),seconds)\n",
    "    print(s)\n",
    "    if outfile is not None:\n",
    "        with open(outfile, 'a') as f:\n",
    "            f.write('Elapsed time: {}\\n'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fastText in dimension 50\n",
      "Elapsed time: 00:02:14.27\n",
      "Running fastText in dimension 100\n",
      "Elapsed time: 00:02:43.26\n",
      "Running fastText in dimension 400\n",
      "Elapsed time: 00:07:13.09\n"
     ]
    }
   ],
   "source": [
    "# Run cbow fastText using hierarchical softmax with Huffman tree\n",
    "for dim in dims:\n",
    "    out_vec_file = save_path + 'vecs_neg_' + str(dim)\n",
    "    results_file = save_path + 'results_neg_' + str(dim) + '.txt'\n",
    "\n",
    "    exec_str = fasttext + ' cbow -input ' + train_file + ' -output ' + out_vec_file + \\\n",
    "              ' -minCount 25 -minn 0 -maxn 0 -t 0.00001 -lr 0.05 -dim ' + str(dim) + ' -ws 10 ' + \\\n",
    "              '-epoch 3 -loss ns -neg 10 -thread 48'\n",
    "\n",
    "    print('Running fastText in dimension {}'.format(dim))\n",
    "    start = time.time()\n",
    "    os.system(exec_str)\n",
    "    elapsed = time.time() - start\n",
    "    with open(results_file, 'w') as f:\n",
    "        f.write(exec_str + '\\n')\n",
    "    print_formated_time(elapsed, results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluation using the word analogy task.\n",
    "def print_accuracy(acc):\n",
    "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
    "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
    "    sem_acc = float(sem_correct)/sem_total\n",
    "    print('Semantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, 100*sem_acc))\n",
    "    \n",
    "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
    "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
    "    syn_acc = float(syn_correct)/syn_total\n",
    "    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%'.format(syn_correct, syn_total, 100*syn_acc))\n",
    "    \n",
    "    total_correct = sem_correct + syn_correct\n",
    "    total = sem_total + syn_total\n",
    "    total_acc = float(total_correct)/total\n",
    "    print('Total: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(total_correct, total, 100*total_acc))\n",
    "    return (sem_acc, syn_acc, total_acc)\n",
    "\n",
    "def evaluate_accuracy(vecs_file):\n",
    "    model = KeyedVectors.load_word2vec_format(vecs_file)\n",
    "    acc = model.accuracy(questions_file)\n",
    "    _, _, total_acc = print_accuracy(acc)\n",
    "    return total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating word analogy task...\n",
      "Dimension: 50\n",
      "Negative sampling:\n",
      "Semantic: 2819/4976, Accuracy: 56.65%\n",
      "Syntactic: 4726/8429, Accuracy: 56.07%\n",
      "Total: 7545/13405, Accuracy: 56.28%\n",
      "\n",
      "Dimension: 100\n",
      "Negative sampling:\n",
      "Semantic: 3689/4976, Accuracy: 74.14%\n",
      "Syntactic: 5699/8429, Accuracy: 67.61%\n",
      "Total: 9388/13405, Accuracy: 70.03%\n",
      "\n",
      "Dimension: 400\n",
      "Negative sampling:\n",
      "Semantic: 4224/4976, Accuracy: 84.89%\n",
      "Syntactic: 6102/8429, Accuracy: 72.39%\n",
      "Total: 10326/13405, Accuracy: 77.03%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating word analogy task...')\n",
    "\n",
    "acc_neg = []\n",
    "\n",
    "for dim in dims:\n",
    "    print('Dimension: {}'.format(dim))\n",
    "    print('Negative sampling:')\n",
    "    acc = evaluate_accuracy(save_path + 'vecs_neg_' + str(dim) + '.vec')\n",
    "    acc_neg.append(acc)\n",
    "    with open(save_path + 'results_neg_' + str(dim) + '.txt', 'a') as f:\n",
    "        f.write('Analogy score: {}\\n'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluation functions for the Stanfor rare words dataset. \n",
    "# Copied from https://github.com/facebookresearch/fastText/blob/master/eval.py\n",
    "def compat_splitting(line):\n",
    "    return line.decode('utf8').split()\n",
    "\n",
    "def similarity(v1, v2):\n",
    "    n1 = np.linalg.norm(v1)\n",
    "    n2 = np.linalg.norm(v2)\n",
    "    return np.dot(v1, v2) / n1 / n2\n",
    "\n",
    "def compute_sim_correlation(vecs_file):\n",
    "    vectors = {}\n",
    "    fin = open(vecs_file, 'rb')\n",
    "    for i, line in enumerate(fin):\n",
    "        try:\n",
    "            tab = compat_splitting(line)\n",
    "            vec = np.array(tab[1:], dtype=float)\n",
    "            word = tab[0]\n",
    "            if not word in vectors:\n",
    "                vectors[word] = vec\n",
    "        except ValueError:\n",
    "            continue\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    fin.close()\n",
    "\n",
    "    mysim = []\n",
    "    gold = []\n",
    "    drop = 0.0\n",
    "    nwords = 0.0\n",
    "\n",
    "    fin = open(dataPath, 'rb')\n",
    "    for line in fin:\n",
    "        tline = compat_splitting(line)\n",
    "        word1 = tline[0].lower()\n",
    "        word2 = tline[1].lower()\n",
    "        nwords = nwords + 1.0\n",
    "\n",
    "        if (word1 in vectors) and (word2 in vectors):\n",
    "            v1 = vectors[word1]\n",
    "            v2 = vectors[word2]\n",
    "            d = similarity(v1, v2)\n",
    "            mysim.append(d)\n",
    "            gold.append(float(tline[2]))\n",
    "        else:\n",
    "            drop = drop + 1.0\n",
    "    fin.close()\n",
    "\n",
    "    corr = stats.spearmanr(mysim, gold)\n",
    "    dataset = os.path.basename(dataPath)\n",
    "    print(\"{}: {}  (OOV: {}%)\"\n",
    "    .format(dataset, corr[0] * 100, math.ceil(drop / nwords * 100.0)))\n",
    "    \n",
    "    return corr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating word similarity task...\n",
      "Dimension: 50\n",
      "Negative sampling:\n",
      "rw.txt: 43.397895191953054  (OOV: 24%)\n",
      "Dimension: 100\n",
      "Negative sampling:\n",
      "rw.txt: 44.95494757620857  (OOV: 24%)\n",
      "Dimension: 400\n",
      "Negative sampling:\n",
      "rw.txt: 47.1176675488831  (OOV: 24%)\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating word similarity task...')\n",
    "\n",
    "corr_neg = []\n",
    "\n",
    "for dim in dims:\n",
    "    print('Dimension: {}'.format(dim))\n",
    "    print('Negative sampling:')\n",
    "    c = compute_sim_correlation(save_path + 'vecs_neg_' + str(dim) + '.vec')\n",
    "    corr_neg.append(c)\n",
    "    with open(save_path + 'results_neg_' + str(dim) + '.txt', 'a') as f:\n",
    "        f.write('Similarity correlation: {}\\n'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plot_results(huffman_score, gmm_score, figure_title, ylabel, ylimits):\n",
    "\n",
    "    pos = range(3)\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "    plt.bar([p - 0.5*width for p in pos],\n",
    "            huffman_score,\n",
    "            width,\n",
    "            alpha=0.5,\n",
    "            color='r')\n",
    "\n",
    "    plt.bar([p + 0.5*width for p in pos],\n",
    "            gmm_score,\n",
    "            width,\n",
    "            alpha=0.5,\n",
    "            color='b')\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xlabel('Embedding dimension')\n",
    "    ax.set_title(figure_title)\n",
    "    ax.set_xticks(pos)\n",
    "    ax.set_xticklabels([50, 100, 400])\n",
    "    plt.xlim(min(pos)-2*width, max(pos)+width*2)\n",
    "    plt.ylim(ylimits)\n",
    "\n",
    "    plt.legend(['Huffman tree', 'GMM tree'], loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#plot_results(acc_huffman, acc_gmm, 'Word analogy task', 'Accuracy', [0, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#plot_results(corr_huffman, corr_gmm, 'Rare words similarity task', 'Correlation', [0, 0.6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
